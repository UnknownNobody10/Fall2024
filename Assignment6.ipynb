{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGyCr8Qh8EfvwD4dVnby6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UnknownNobody10/Fall2024/blob/main/Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBE0qeL2qyDL"
      },
      "outputs": [],
      "source": [
        "#Step 1: Install Dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "#Step 2: Add environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.3.0-bin-hadoop3\"\n",
        "os.environ[\"HADOOP_HOME\"] = os.environ[\"SPARK_HOME\"]\n",
        "\n",
        "#these are new enviromental variables\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell\"\n",
        "#Step 3: Initialize Pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating spark context\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "id": "JuDWi8QOq13b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.graphx._\n",
        "import org.apache.spark.rdd.RDD\n",
        "import org.apache.spark.graphx.GraphLoader\n",
        "import org.apache.spark.graphx.Graph\n",
        "\n",
        "val vertices: RDD[(VertexId, (String, Int))] = sc.parallelize(Seq(\n",
        "    (1L, (\"Aron\", 350)),\n",
        "    (2L, (\"Bill\", 360)),\n",
        "    (3L, (\"Claire\", 195)),\n",
        "    (4L, (\"Daniel\", 90)),\n",
        "    (5L, (\"Eric\", 90)),\n",
        "    (6L, (\"Frank\", 215)),\n",
        "    (7L, (\"Graham\", 30)),\n",
        "    (8L, (\"Henry\", 25)),\n",
        "    (9L, (\"Inna\", 25)),\n",
        "    (10L, (\"Jen\", 20))\n",
        "))\n",
        "\n",
        "val edges: RDD[Edge[Int]] = sc.parallelize(Seq(\n",
        "    Edge(1L, 2L, 60),\n",
        "    Edge(2L, 1L, 50),\n",
        "    Edge(1L, 3L, 50),\n",
        "    Edge(3L, 1L, 100),\n",
        "    Edge(1L, 4L, 90),\n",
        "    Edge(3L, 9L, 25),\n",
        "    Edge(3L, 10L, 20),\n",
        "    Edge(2L, 6L, 50),\n",
        "    Edge(6L, 2L, 110),\n",
        "    Edge(6L, 7L, 30),\n",
        "    Edge(6L, 8L, 25),\n",
        "    Edge(2L, 5L, 90)\n",
        "))\n",
        "\n",
        "val graph: Graph(vertices, edges)\n",
        "\n",
        "\n",
        "# Print the vertices and edges\n",
        "println(\"Vertices: \")\n",
        "graph.vertices.collect.foreach {case (id, (name, total_seconds)) =>\n",
        "  println(s\"Vertex ID: $id, Name: $name, Total Seconds: $total_seconds\")\n",
        "}\n",
        "\n",
        "println(\"\\nEdges: \")\n",
        "graph.edges.collect.foreach { edge =>\n",
        "  println(s\"Source: $(edge.srcID}, Destination: ${edge.dstID}, Seconds: ${edge.attr}\")\n",
        "}\n",
        "\n",
        "#degrees, in-degrees, out-degrees\n",
        "val degrees = graph.degrees\n",
        "val inDegrees = graph.inDegrees\n",
        "val outDegrees = graph.outDegrees\n",
        "\n",
        "#BFS\n",
        "val frankId = vertices.filter { case (_, (name, _)) => name == \"Frank\" }.first()._1\n",
        "val targetVertices = vertices.filter { case (_, (_, seconds)) => seconds > 215}.map(_._1).collect.toSet\n",
        "val bfs = graph.bfs(frankId, targetVertices)\n"
      ],
      "metadata": {
        "id": "OuxJA6I7q6C8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}